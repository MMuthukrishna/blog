<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.101.0"><title>DAG Workflow Using AWS Step Functions and AWS Lambda - MK</title><meta property="og:title" content="DAG Workflow Using AWS Step Functions and AWS Lambda - MK"><link href=https://mmuthukrishna.github.io/blog/index.xml rel=alternate type=application/rss+xml title=MK><link rel=stylesheet href=https://mmuthukrishna.github.io/blog/css/open-color.css media=all><link rel=stylesheet href=https://mmuthukrishna.github.io/blog/css/main.css media=all><link rel=stylesheet href=https://mmuthukrishna.github.io/blog/css/fonts.css></head><body><div class=wrapper><header class=header><nav class=nav><a href=https://mmuthukrishna.github.io/blog/ class=nav-logo><img src=https://mmuthukrishna.github.io/blog/images/logo.jpg width=50 height=50 alt=Logo></a><ul class=nav-links><li><a href=../../categories>Categories</a></li><li><a href=../../tags>Tags</a></li><li><a href=../../about/>About</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title>DAG Workflow Using AWS Step Functions and AWS Lambda</h1><div class=article-content><p>At Instamojo, we use Amazon Redshift as the data warehouse.</p><p>A typical requirement for analytics team is to aggregate new data
from multiple source tables (say raw events) that can later be used
in BI tools, dashboards, charts or other machine learning jobs.</p><p>For the sake of simplicity, let&rsquo;s assume these are the aggregate
operations performed</p><p>$$ f = MIN(x) $$<br>$$ f = MAX(x) $$<br>$$ f = COUNT(x) $$<br>$$ f = SUM(x) $$</p><p>and that there are $$ N_{agg} $$ aggregate tasks which either append
or reload a table in aggregate_schema (example: <code>agg.table_one</code>).</p><p>These $$ N_{agg} $$ aggregate tasks have a hierarchy.</p><p>I will explain this with an example.</p><p>Consider Task $$ T_{i} $$ which updates <code>agg.table_i</code> by executing a SQL query, whose results are either appended to <code>agg.table_i</code> or truncated and appended to <code>agg.table_i</code> (reload).</p><p>Let the SQL query executed by Task $$ T_{i} $$ be</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#6ab825;font-weight:700>select</span><span style=color:#666>
</span></span></span><span style=display:flex><span><span style=color:#666></span>...<span style=color:#666>
</span></span></span><span style=display:flex><span><span style=color:#666></span>...<span style=color:#666>
</span></span></span><span style=display:flex><span><span style=color:#666></span>...<span style=color:#666>
</span></span></span><span style=display:flex><span><span style=color:#666></span><span style=color:#6ab825;font-weight:700>from</span><span style=color:#666> </span>agg.table_j<span style=color:#666>
</span></span></span><span style=display:flex><span><span style=color:#666></span>...<span style=color:#666>
</span></span></span></code></pre></div><p>Here Task $$T_{i}$$ queries aggregate table <code>agg.table_j</code>.</p><p>The aggregate table <code>agg.table_j</code> will be updated by a Task $$T_{j}$$.<br>So the task $$T_{j}$$ has to be executed before $$T_{i}$$.</p><p>This is a simple example, in reality there are aggregate tasks whose queries
depend of 12 other aggregate tables.</p><p>We can generate the task hierarchy by parsing the queries.</p><p>You can just run a simple script on each query</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>grep -oP <span style=color:#ed9d13>&#39;(?&lt;=agg.)\w+&#39;</span> {file} | tr -s <span style=color:#ed9d13>&#39; &#39;</span> | sort --unique
</span></span></code></pre></div><p>To get the list of tables and tasks that a given aggregate task (query executor) depends on</p><p>Running it on the above query, we get the output as</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>table_j
</span></span></code></pre></div><p>If we represent the task hierarchy using a Directed Acyclic Graph (DAG) then there will be a directed edge from Task $$T_{j}$$ to Task $$T_{i}$$.</p><p>$$T_{j}$$ &mdash;> $$T_{i}$$</p><p>If we parse all $$N_{agg}$$ aggregate queries and plot the Directed Acyclic Graph using networkx and pyvis. It would look something like this</p><p><img src=../../image/dag-workflow-using-aws-step/original_dag.png alt=original_dag></p><p>I have replaced the task names with numbers as I&rsquo;m not allowed to reveal the name of the tasks.</p><p>From the above directed acyclic graph, you can see that all nodes are connected to $$T_{27}$$, which is the end node $$fin:end$$. The reason for doing this will be revealed later.</p><h3 id=transitive-reduction>Transitive reduction</h3><p>I will explain what transitive reduction of a Directed Acyclic Graph is, with an example</p><p>Consider the graph shown below</p><p><img src=../../image/dag-workflow-using-aws-step/example_dag_with_transitive_edges.png alt=example_dag_with_transitive_edges></p><p>Task $$T_4$$ depends on $$T_1$$, $$T_2$$ and $$T_6$$.<br>The directed edge from $$T_1$$ to $$T_4$$ is redundant information.<br>As we already have a directed edge from $$T_1$$ to $$T_2$$ and $$T_2$$ to $$T_4$$.<br>It is implied that $$T_4$$ depends on $$T_1$$ and $$T_4$$ will be executed after $$T_1$$ even if we did not have that directed edge from $$T_1$$ to $$T_4$$.</p><p>To remove such edges, networkx has a method called nx.transitive_reduction</p><div class=highlight><pre tabindex=0 style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>REDUCED_G = nx.transitive_reduction(G)
</span></span></code></pre></div><p>Applying transitive reduction to the above simple example graph, we get</p><p><img src=../../image/dag-workflow-using-aws-step/example_dag_after_transitive_reduction.png alt=example_dag_after_transitive_reduction></p><p>The directed edge from $$T_1$$ to $$T_4$$ and $$T_1$$ to $$T_5$$ are removed after transitive reduction.</p><p>Applying transitive reduction on the original graph, we get</p><p><img src=../../image/dag-workflow-using-aws-step/original_dag_after_transitive_reduction.png alt=original_dag_after_transitive_reduction></p><p>At this point, you can either write a script that generates Apache Airflow code or do some more work to actually get it running in AWS Step.</p><p>If you observe the above graphs, they all seem to look like Multistage graphs, except they&rsquo;re not exactly Multistage graphs, as Mutistage graphs expects there to be edges only from one stage to the next stage.</p><p>There are edges here from stage 1 to stage 3, 4 &mldr; N</p><p>After transitive reduction, only few nodes are connected to $$T_{27}$$, these are the tasks which populate the aggregate schema tables that isn&rsquo;t used by any other task. We could refer to them as unfruitful tasks.</p><p>Let&rsquo;s define few terminologies</p><h3 id=level-of-a-node>Level of a node</h3><p>Level of a node is the length of the longest path from all root nodes to a specific node.</p><p><img src=../../image/dag-workflow-using-aws-step/level_graph_modified.png alt=level_graph_modified></p><p>$$ L_1 = {1} $$<br>$$ L_2 = {2,3} $$<br>$$ L_3 = {4} $$<br>$$ L_4 = {5} $$</p><p>We can further divide each level into groups based on the highest level of all child nodes of a given node</p><p>$$ N \in L_{ij} \implies N \in L_i \quad \land \quad min(k) = j \quad \forall M \in L_k , (N, M) \in E $$</p><p>I will explain this, with an example</p><p><img src=../../image/dag-workflow-using-aws-step/example_dag_after_transitive_reduction.png alt=example_dag_after_transitive_reduction></p><p>$$ L_{12} = {1} $$<br>$$ L_{13} = {6} $$<br>$$ L_{23} = {2} $$<br>$$ L_{24} = {3} $$<br>$$ L_{34} = {4} $$<br>$$ L_{4E} = {5} $$</p><p>In order to execute all such aggregate tasks, you can either do a simple level order traversal where tasks in each level can be executed using a Map state in AWS Step with desired MaxConcurrency.</p><h3 id=simple-level-order>Simple Level Order</h3><p><img src=../../image/dag-workflow-using-aws-step/simple_level_order.png alt=simple_level_order></p><p>The state machine definition shown above is using a pass state, which should be replaced by a Map state (which will execute all aggregate tasks in array $$L_{i}$$ with desired MaxConcurrency).</p><p>This is a simple level order traversal of the Directed Acyclic Graph</p><p>Or you could do this</p><h3 id=parent-level-order>Parent Level Order</h3><p><img src=../../image/dag-workflow-using-aws-step/divided_level_order.png alt=divided_level_order></p><p>In this approach we are doing a level order traversal of parent nodes of each level.</p><p>$$L_{12}$$ has all parents of $$L_{2}$$<br>$$L_{13}$$ and $$L_{23}$$ has all parents of $$L_3$$</p><p>$$L_{1E}$$ &mldr; $$L_{5E}$$ are all unfruitful tasks.</p><p>Any of the above two approaches will obey the hierarchy defined by the Directed Acyclic Graph.</p><p>My intuition is that Parent Level Order traversal has higher parallelism.</p><p>It&rsquo;s left to you to choose any one of the techniques to execute the aggregate tasks defined in DAG.</p><p>The title says DAG Workflow Using AWS Step Functions and AWS Lambda
The aggregate tasks usually take longer than 900 seconds, so how would we execute it with AWS Lambda</p><p>You can use the newly introduced <a href=https://aws.amazon.com/about-aws/whats-new/2020/09/announcing-data-api-for-amazon-redshift/>Redshift Data API</a> and a wait state loop.</p><p><img src=../../image/dag-workflow-using-aws-step/wait_state_loop.png alt=wait_state_loop></p><p>You can run UNLOAD asynchronously and transition to a wait state with predefined wait time in seconds (or you can go as far has having wait time defined in a config file for each aggregate task in S3 which is updated regularly at the end of the DAG workflow based on the actual time it took to run each aggregate task, that seems overkill, but possible, for now we will stick to constant wait time defined in a config file) and then check for the status of UNLOAD query execution.</p><p>Similarly, we execute COPY asynchronously to load data from S3 into agg schema tables.</p><p>The actual State Machine Defintion looks something like this</p><p><img src=../../image/dag-workflow-using-aws-step/actual_state_machine.png alt=actual_state_machine></p><p>where the Pass State is replaced with an Aggregate Task Map State.</p><h3 id=sidenote>Sidenote</h3><p><img src=../../image/dag-workflow-using-aws-step/side_note_image.png alt=sidenote></p><h3 id=reasons-for-using-aws-step>Reasons for using AWS Step</h3><ul><li>I completed this project before Managed Airflow was launched by AWS in reInvent 2020.</li><li>AWS Step has a simple regulator, MaxConcurrency, which can set so as to ensure the total number of connections used is within the connection limit of Redshift (you can write a simple script to calculate the max number of connections used in parent level order traversal, it&rsquo;s not that hard).</li></ul></div><ul class=article-taxonomy><hr><li><i class="fa fa-category"></i><a href=../../categories/aws>aws</a></li><li><i class="fa fa-tags"></i><a href=../../tags/aws>aws</a><a href=../../tags/step-functions>step-functions</a></li></article></main><footer class=footer><ul class=footer-links><li><a href=https://mmuthukrishna.github.io/blog/index.xml type=application/rss+xml target=_blank><i class="fa fa-rss"></i>
RSS feed</a></li><li><a href=../../>Home</a></li><li><a href=https://mmuthukrishna.github.io/blog/site-notice>Site notice</a></li></ul></footer></div></body><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css integrity=sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js integrity=sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js integrity=sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF crossorigin=anonymous onload=renderMathInElement(document.body)></script></html>